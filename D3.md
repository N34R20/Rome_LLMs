# Lecture 3

## Autodifferentiation

- basics of autograd

https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html

keeping track of gradient has a cost, only useful while training

```python
import torch
x = torch.tensor([2.,3.], requires_grad=True)
# now the tensor has gradients
y = x + 2
# Addbackward
z = y * y * 3
# Mulbackward
out = z.mean()
# Meanbackward

# computing gradients

out.backward()
x.grad

# output
tensor([12.,15.])

# stop gradient tracking

with torch.no_grad():
    y = x + 2
```


