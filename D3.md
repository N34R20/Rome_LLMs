# Lecture 3

## Autodifferentiation

- basics of autograd

https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html

keeping track of gradient has a cost, only useful while training

```python
import torch
x = torch.tensor([2.,3.], requires_grad=True)
# now the tensor has gradients
y = x + 2
# Addbackward
z = y * y * 3
# Mulbackward
out = z.mean()
# Meanbackward

# computing gradients

out.backward()
x.grad

# output
tensor([12.,15.])

# stop gradient tracking

with torch.no_grad():
    y = x + 2
```


# Transformers

## Attention

https://en.wikipedia.org/wiki/Attention_(machine_learning)

The machine learning-based attention method simulates how human attention works by assigning varying levels of importance to different components of a sequence. In natural language processing, this usually means assigning different levels of importance to different words in a sentence. It assigns importance to each word by calculating "soft" weights for the word's numerical representation, known as its embedding, within a specific section of the sentence called the context window to determine its importance. The calculation of these weights can occur simultaneously in models called transformers, or one by one in models known as recurrent neural networks. Unlike "hard" weights, which are predetermined and fixed during training, "soft" weights can adapt and change with each use of the model.


- Tokenization

![attention-qkv](/imgs/Attention-qkv.png)


$$ Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt d_K}) V $$
$$ MultiHeadedAttention(Q,K,V) = Concat(head_1, ... , head_h) W^O $$
$$ Q =  XW $$
$$ K =  XW^K $$
$$ V =  XW^V $$