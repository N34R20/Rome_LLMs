# Lecture 1

### Bases
- Linear Regression
- Logistic Regression
- MLP / Backpropagation

- Lenguage models (Word2Vec)

## Linear Regression

The building block of any llm

Learn the relationships between these two variables $(x_i, y_i)$

$y = \alpha * x + b$

The parameters $\alpha$ and $b$ are acting linearly on aour variables.

What is the BEST line? 
A good line (model) should minimize some sort of loss function

The most popular loss is the Mean Squared Error

$$ MSE = \frac{1}{n}  \sum^{n}_{i=1}(y_i - \hat{y}_i)^2 $$

- $n$ : number of samples 

- $y_i$ : real values

- $\hat{y}_i$ : predicted values 

$$ Loss = \frac{1}{n}  \sum^{n}_{i=1}(y_i - (\alpha*x_i + b))^2 $$



$
Y = 
\begin{bmatrix}
    1 \\
    1 \\
    1 \\
    ...
\end{bmatrix}
$
$
X = 
\begin{bmatrix}
    1 & 0   &  \\
    1   &  &   \\
    1   & 0   & 1 \\
    ...
\end{bmatrix}
$
$
\beta = 
\begin{bmatrix}
    1 \\
    1 \\
    1 \\
    ...
\end{bmatrix}
$

$$ L = (Y - X\beta)^T (Y - X\beta) $$
$$ L = Y^T Y - Y^T X\beta + Y - X\beta) $$
$$ L = (Y - X\beta)^T (Y - X\beta) $$
$$ \frac{\partial L}{\partial \beta} = $$

$$ \beta = (X^T X)^{-1} X^T Y$$


## Gradient Descent

- $y - \alpha x + b$ : residual
$$ \frac{\partial L}{\partial \alpha} = (-x)(y - \alpha x + b)$$

## Bias - Variance Tradeoff